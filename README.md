# prob_robotics_demo
確率ロボティクス課題提出用リポジトリ

このリポジトリは、確率ロボティクスにおける課題提出用のアルゴリズムを載せています。各アルゴリズムの数式と、その実行結果をアニメーションで確認できます。

## 数式

### 1. 自己位置推定 (Localization)

#### **カルマンフィルタ (Kalman Filter)**
線形ガウスシステムにおいて、状態 $x_t$ の事後分布 $p(x_t | y_{1:t})$ を解析的に求めます。
- **予測ステップ**: 
  $$\hat{x}_t^- = F \hat{x}_{t-1} + B u_t$$
  $$P_t^- = F P_{t-1} F^T + Q$$
- **更新ステップ**: 
  $$K_t = P_t^- H^T (H P_t^- H^T + R)^{-1}$$
  $$\hat{x}_t = \hat{x}_t^- + K_t (y_t - H \hat{x}_t^-)$$
  $$P_t = (I - K_t H) P_t^-$$

#### **MCL (Monte Carlo Localization)**
パーティクルフィルタを用いた非線形・非ガウスな推定手法です。
- **重み更新**: 各パーティクル $x_t^{(i)}$ の重み $w_t^{(i)}$ は、観測モデル $p(y_t | x_t)$ に基づいて計算されます。
  $$w_t^{(i)} \propto w_{t-1}^{(i)} \cdot p(y_t | x_t^{(i)})$$

---

### 2. クラスタリングと混合モデル

#### **EMアルゴリズム (Expectation-Maximization)**
混合ガウス分布 (GMM) の対数尤度を最大化します。
- **Eステップ (負担率)**: 
  $$\gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}$$
- **Mステップ (パラメータ更新)**: 
  $$\mu_k^{new} = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} x_n, \quad \Sigma_k^{new} = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} (x_n - \mu_k)(x_n - \mu_k)^T$$

#### **変分推論 (Variational Inference)**
真の事後分布 $p(Z, \pi, \mu | X)$ を近似分布 $q(Z)q(\pi)q(\mu)$ で近似します。本デモでは、混合比 $\pi$ にディリクレ分布 $Dir(\pi | \alpha)$ を仮定しています。
- **自動モデル選択**: データが割り当てられないクラスタ $k$ は、事後パラメータ $\alpha_k$ が事前分布の値 $\alpha_0$ に収束し、混合比の期待値 $E[\pi_k] \to 0$ となり、事実上「消滅」します。

---

### 3. ベイズ回帰 (Bayesian Regression)

重みパラメータ $w$ の事後分布 $p(w | X, y)$ を求めます。
- **事後分布の共分散と平均**:
  $$S_N = (\alpha I + \beta \Phi^T \Phi)^{-1}$$
  $$m_N = \beta S_N \Phi^T y$$
- **予測分布**: 未知の入力 $x^*$ に対して、予測値は以下のガウス分布に従います。
  $$p(t | x^*, X, y) = \mathcal{N}(t | m_N^T \phi(x^*), \sigma_N^2(x^*))$$
  ここで、$\sigma_N^2(x^*) = \frac{1}{\beta} + \phi(x^*)^T S_N \phi(x^*)$ であり、データが増えるほど第2項（モデルの不確実性）が減少します。

---

## 各アルゴリズムの処理フロー

各アニメーションで実行されている計算の具体的なプロセスを解説します。

### 1. 自己位置推定 (Localization)

#### **MCL (Monte Carlo Localization)**
1. **予測 (Prediction)**: 全てのパーティクルをロボットの移動指令 $u_t$ に基づいて動かします。この際、移動誤差を想定したノイズを加えるため、目印がない間は移動誤差を想定してパーティクルが広がっていきます。
2. **計測更新 (Measurement Update)**: ロボットの進行方向を基準とした**視野角（FOV）および最大計測距離**の範囲内にランドマークが存在する場合のみ実行されます。各パーティクルの位置で「ランドマークがどう見えるか」を計算し、実際の観測値との類似度を重み $w$ として割り当てます。
3. **リサンプリング (Resampling)**: 重み $w$ に比例した確率でパーティクルを復元抽出します。これにより、視野内のランドマークと矛盾する位置にいたパーティクルが淘汰され、尤度の高い場所にパーティクルが「ギュッ」と集まります。


#### **カルマンフィルタ (Kalman Filter)**
1. **予測**: 前時刻の推定値（平均 $\mu$, 分散 $\Sigma$）から、移動後の分布を計算します。移動により不確実性（分散）が増大し、円が広がります。
2. **更新**: **ロボットの視野（黄色の扇形）**にランドマークが入った際に観測値を得ます。予測値と観測値の「確からしさ」を比較（カルマンゲイン $K_t$ の計算）し、最も尤もらしい位置へ推定値を修正します。このとき不確実性が減少し、円が収縮します。

---

### 2. クラスタリングと混合モデル

#### **k-means**
1. **割り当て**: 各データ点に対し、最も近い中心点（セントロイド）のクラスタラベルを付与します。
2. **更新**: クラスタごとに属するデータの平均座標を計算し、そこを新しい中心点とします。これを収束まで繰り返します。

#### **EMアルゴリズム (Expectation-Maximization)**
1. **Eステップ**: 各データ点が各ガウス分布から生成された確率（負担率）を計算します。
2. **Mステップ**: 負担率を重みとして、各ガウス分布の平均、共分散行列、および混合比を再計算し、データにフィッティングさせます。

#### **変分推論 (Variational Inference)**
1. **負担率の計算**: EM法と同様ですが、パラメータ自体も確率分布として扱います。
2. **事後分布の更新**: 観測データに基づいてディリクレ分布（混合比）やガウス分布（平均・精度）のパラメータを更新します。
3. **自動モデル選択 (Pruning)**: データが全く割り当てられないクラスタは、事前分布のパラメータ $\alpha_0$ の影響が支配的になり、混合比 $\pi$ が実質的に 0 へ収束して消滅します。

---

### 3. ベイズ回帰 (Bayesian Regression)

1. **事前分布の設定**: 重み $w$ に対して、平均 0、分散の大きいガウス分布を仮定します。
2. **逐次学習**: 新しいデータ $(x, y)$ が得られるたびに、尤度関数を掛け合わせて事後分布を更新します。
3. **予測分布の算出**: 重みの不確実性を積分消去し、予測値 $y$ 自体をガウス分布として出力します。データが少ない領域では「自信がない（分散が大きい）」という結果が返るのが特徴です。

---
## 実行方法

1. 必要なライブラリをインストールします。
   ```bash
   pip install -r requirements.txt
2.```Probabilistic_Robotics_Demo.ipynb``` を開き、全てのセルを実行してください。

3.```HTML(ani_obj.to_jshtml())``` を含むセルで、アニメーションが表示されます。
## 📖 参考文献
- [上田隆一: 詳解 確率ロボティクス Pythonによる基礎アルゴリズムの実装](https://github.com/ryuichiueda/LNPR_BOOK_CODES)
- [講義資料](https://github.com/ryuichiueda/slides_marp/blob/master/prob_robotics_2025/)
